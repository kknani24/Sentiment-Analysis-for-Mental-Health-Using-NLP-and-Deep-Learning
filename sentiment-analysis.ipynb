{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import plotly.figure_factory as ff\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path = 'Combined Data.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# EDA\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target labels\n",
    "fig = px.histogram(df, x='status', title='Distribution of Mental Health Status')\n",
    "fig.show()\n",
    "\n",
    "# Handle NaN values in the statement column\n",
    "df['statement'] = df['statement'].fillna('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text lengths\n",
    "df['text_length'] = df['statement'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Create the histogram\n",
    "fig = px.histogram(df, x='text_length', title='Text Length Distribution', nbins=50, template='plotly_dark')\n",
    "\n",
    "# Calculate mean and median\n",
    "mean_text_length = df['text_length'].mean()\n",
    "median_text_length = df['text_length'].median()\n",
    "\n",
    "# Add mean and median lines\n",
    "fig.add_vline(x=mean_text_length, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Mean\", annotation_position=\"top left\")\n",
    "fig.add_vline(x=median_text_length, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Median\", annotation_position=\"top right\")\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    xaxis_title='Text Length (Number of Words)',\n",
    "    yaxis_title='Count',\n",
    "    title_x=0.5,\n",
    "    bargap=0.2,\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=mean_text_length,\n",
    "            y=max(df['text_length']),\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            text='Mean',\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            ax=20,\n",
    "            ay=-30\n",
    "        ),\n",
    "        dict(\n",
    "            x=median_text_length,\n",
    "            y=max(df['text_length']),\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            text='Median',\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            ax=-20,\n",
    "            ay=-30\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\n', '', text)  # Remove newlines\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
    "    return text\n",
    "\n",
    "df['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Tokenization and Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "def augment_text(text):\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        translated = blob.translate(to='fr').translate(to='en')\n",
    "        return str(translated)\n",
    "    except Exception as e:\n",
    "        return text\n",
    "\n",
    "df['augmented_statement'] = df['statement'].apply(augment_text)\n",
    "augmented_df = df[['statement', 'status']].copy()\n",
    "augmented_df['statement'] = df['augmented_statement']\n",
    "df = pd.concat([df, augmented_df])\n",
    "\n",
    "# Reapply preprocessing on augmented data\n",
    "df['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))\n",
    "df['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Ensure no NaN values are left\n",
    "df['cleaned_statement'] = df['cleaned_statement'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X = df['cleaned_statement']\n",
    "y = df['status']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_seq, maxlen=100)\n",
    "\n",
    "# Convert labels to integers\n",
    "label_map = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y_int = y.map(label_map)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_int, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128))  # Removed input_length\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_map), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred_classes))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "cm_fig = ff.create_annotated_heatmap(\n",
    "    z=cm,\n",
    "    x=list(label_map.keys()),\n",
    "    y=list(label_map.keys()),\n",
    "    annotation_text=cm,\n",
    "    colorscale='Viridis'\n",
    ")\n",
    "cm_fig.update_layout(title='Confusion Matrix')\n",
    "cm_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "all_text = ' '.join(df['cleaned_statement'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Cleaned Statements')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Distribution\n",
    "fig = px.pie(df, names='status', title='Proportion of Each Status Category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_status(statement, tokenizer, model, label_map, reverse_label_map):\n",
    "    # Preprocess the input statement\n",
    "    processed_statement = preprocess_text(statement)\n",
    "    processed_statement = remove_stopwords(processed_statement)\n",
    "    \n",
    "    # Tokenize and pad the statement\n",
    "    sequence = tokenizer.texts_to_sequences([processed_statement])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=100)\n",
    "    \n",
    "    # Predict the status\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "    # Map the predicted class to the status label\n",
    "    predicted_status = reverse_label_map[predicted_class]\n",
    "    \n",
    "    return predicted_status\n",
    "\n",
    "# Create reverse label map for decoding\n",
    "reverse_label_map = {idx: label for label, idx in label_map.items()}\n",
    "\n",
    "# Example usage:\n",
    "statement_to_predict = 'trouble sleeping, confused mind, restless heart. All out of tune'\n",
    "predicted_status = predict_status(statement_to_predict, tokenizer, model, label_map, reverse_label_map)\n",
    "print(f\"The predicted status for the given statement is: {predicted_status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "statement_to_predict = 'I have nothing left. I do not want to be here anymore, I want to be free, I want to go. I received no help from my psychiatrist nor from my doctor when I told them about my depression because I am 16 in the middle of a global pandemic so they think I am just sad like everyone else. I am tired of receiving no help whatsoever. I needed it but now, it is too late. I just want to die already. I have had anxiety for my whole life and was diagnosed with it 3 years ago. But now, the consequences are too important. I just want some rest god dammit. My parents do not care, they do not want to help me and my friends are mean to me for no reason.'\n",
    "predicted_status = predict_status(statement_to_predict, tokenizer, model, label_map, reverse_label_map)\n",
    "print(f\"The predicted status for the given statement is: {predicted_status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
